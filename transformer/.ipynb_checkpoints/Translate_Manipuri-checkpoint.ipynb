{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2208f235-83c8-4786-8fac-53dfbd0e72e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 15:17:00.671886: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 15:17:00.671908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 15:17:00.672754: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 15:17:00.677631: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 15:17:01.246075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93a8091-0bcb-4705-a34a-9e915dd00a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied /home/surchand/projects/TranslateManipuri/data/master_data.xls\n",
      "Read 20019 rows, 21449 lines from data sheet 1\n",
      "Read 20034 rows, 22076 lines from data sheet 2\n",
      "Read 10111 rows, 11179 lines from data sheet 3\n",
      "Read 6540 rows, 1159 lines from data sheet 4\n",
      "File copied /home/surchand/projects/TranslateManipuri/data/master_data.xls\n",
      "Read 20019 rows, 21449 lines from data sheet 1\n",
      "Read 20034 rows, 22076 lines from data sheet 2\n",
      "Read 10111 rows, 11179 lines from data sheet 3\n",
      "Read 6540 rows, 1159 lines from data sheet 4\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/home/surchand/projects/TranslateManipuri/src\")\n",
    "from excel_to_text import generateData\n",
    "english_se, manipuri_se = generateData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27fbd12-9909-482c-bedb-3fd094a06986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55863"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manipuri_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302abf34-7714-4df7-b4ca-11973c2e5b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 15:17:35.937669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 15:17:35.968078: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 15:17:35.968248: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 15:17:35.969443: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 15:17:35.969588: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 15:17:35.969719: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 15:17:36.021441: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 15:17:36.021609: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 15:17:36.021702: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-01-12 15:17:36.021751: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-12 15:17:36.021849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6006 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "train_val_pc = 0.95\n",
    "total_data = len(manipuri_se)\n",
    "split_len = tf.cast(total_data * train_val_pc, tf.int32)\n",
    "split_len\n",
    "train_mn, val_mn = manipuri_se[:split_len], manipuri_se[split_len:]\n",
    "train_en, val_en = english_se[:split_len], english_se[split_len:]\n",
    "train_examples = tf.data.Dataset.from_tensor_slices((train_en, train_mn))\n",
    "val_examples = tf.data.Dataset.from_tensor_slices((val_en, val_mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efaa1610-49e6-48dd-9335-b286e46fe0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Examples in Manipuri:\n",
      "ꯑꯩ ꯄꯨꯡ ꯑꯃ ꯕꯁ ꯉꯥꯢꯔꯛꯂꯤ ꯫\n",
      "ꯇꯣꯝꯒꯤ ꯁꯥꯟꯅꯃꯤꯟꯅꯅꯕ ꯃꯃꯥꯟꯅꯕ ꯑꯃꯇ ꯂꯩꯇꯦ ꯫\n",
      "ꯑꯩꯅ ꯇꯣꯝ ꯆꯠꯍꯟꯈꯤ ꯫\n",
      "ꯇꯥꯢꯌꯔꯗꯨ ꯅꯨꯡꯁꯤꯠ ꯆꯠꯂꯤ ꯫\n",
      "ꯇꯣꯝꯅ ꯃꯍꯥꯛ ꯏꯅꯥꯛꯈꯨꯟꯕ ꯅꯤꯄꯥ ꯑꯃꯒꯤ ꯃꯆꯥꯅꯤ ꯍꯥꯢꯈꯤ ꯫\n",
      "\n",
      "> Examples in English:\n",
      "I've been waiting for the bus for an hour .\n",
      "Tom doesn't have any friends to play with .\n",
      "I made Tom go .\n",
      "The tire leaks air .\n",
      "Tom claimed he was the son of a rich man .\n"
     ]
    }
   ],
   "source": [
    "for en_examples, mn_examples in train_examples.batch(5).take(1):\n",
    "  print('> Examples in Manipuri:')\n",
    "  for mn in mn_examples.numpy():\n",
    "    print(mn.decode('utf-8'))\n",
    "  print()\n",
    "\n",
    "  print('> Examples in English:')\n",
    "  for en in en_examples.numpy():\n",
    "    print(en.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92995cbc-a48e-4026-b073-6754657e93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_translate_mn_en_converter'\n",
    "tokenizers = tf.saved_model.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d2624b-6a84-4ec0-bce4-60693c2ed104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detokenize',\n",
       " 'get_reserved_tokens',\n",
       " 'get_vocab_path',\n",
       " 'get_vocab_size',\n",
       " 'lookup',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in dir(tokenizers.mn) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0308abac-c35a-4859-893f-ca75e99ce7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This is a batch of strings:\n",
      "ꯑꯩ ꯄꯨꯡ ꯑꯃ ꯕꯁ ꯉꯥꯢꯔꯛꯂꯤ ꯫\n",
      "ꯇꯣꯝꯒꯤ ꯁꯥꯟꯅꯃꯤꯟꯅꯅꯕ ꯃꯃꯥꯟꯅꯕ ꯑꯃꯇ ꯂꯩꯇꯦ ꯫\n",
      "ꯑꯩꯅ ꯇꯣꯝ ꯆꯠꯍꯟꯈꯤ ꯫\n",
      "ꯇꯥꯢꯌꯔꯗꯨ ꯅꯨꯡꯁꯤꯠ ꯆꯠꯂꯤ ꯫\n",
      "ꯇꯣꯝꯅ ꯃꯍꯥꯛ ꯏꯅꯥꯛꯈꯨꯟꯕ ꯅꯤꯄꯥ ꯑꯃꯒꯤ ꯃꯆꯥꯅꯤ ꯍꯥꯢꯈꯤ ꯫\n"
     ]
    }
   ],
   "source": [
    "print('> This is a batch of strings:')\n",
    "for mn in mn_examples.numpy():\n",
    "  print(mn.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0413ab04-5fe0-4cd4-a18e-85da8717a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This is a padded-batch of token IDs:\n",
      "[2, 93, 212, 101, 610, 3402, 80, 3]\n",
      "[2, 121, 884, 517, 701, 2445, 1438, 131, 153, 80, 3]\n",
      "[2, 97, 94, 1881, 80, 3]\n",
      "[2, 375, 2275, 123, 661, 262, 80, 3]\n",
      "[2, 96, 98, 1287, 551, 406, 630, 163, 80, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizers.mn.tokenize(mn_examples)\n",
    "\n",
    "print('> This is a padded-batch of token IDs:')\n",
    "for row in encoded.to_list():\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f081ddd1-72ec-44a8-a73c-3b25554e7920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This is human-readable text:\n",
      "ꯑꯩ ꯄꯨꯡ ꯑꯃ ꯕꯁ ꯉꯥꯢꯔꯛꯂꯤ ꯫\n",
      "ꯇꯣꯝꯒꯤ ꯁꯥꯟꯅꯃꯤꯟꯅꯅꯕ ꯃꯃꯥꯟꯅꯕ ꯑꯃꯇ ꯂꯩꯇꯦ ꯫\n",
      "ꯑꯩꯅ ꯇꯣꯝ ꯆꯠꯍꯟꯈꯤ ꯫\n",
      "ꯇꯥꯢꯌꯔꯗꯨ ꯅꯨꯡꯁꯤꯠ ꯆꯠꯂꯤ ꯫\n",
      "ꯇꯣꯝꯅ ꯃꯍꯥꯛ ꯏꯅꯥꯛꯈꯨꯟꯕ ꯅꯤꯄꯥ ꯑꯃꯒꯤ ꯃꯆꯥꯅꯤ ꯍꯥꯢꯈꯤ ꯫\n"
     ]
    }
   ],
   "source": [
    "round_trip = tokenizers.mn.detokenize(encoded)\n",
    "\n",
    "print('> This is human-readable text:')\n",
    "for line in round_trip.numpy():\n",
    "  print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac1db42-b63a-4eed-99c3-f99a78e3eeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This is the text split into tokens:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'\\xea\\xaf\\x91\\xea\\xaf\\xa9',\n",
       "  b'\\xea\\xaf\\x84\\xea\\xaf\\xa8\\xea\\xaf\\xa1', b'\\xea\\xaf\\x91\\xea\\xaf\\x83',\n",
       "  b'\\xea\\xaf\\x95\\xea\\xaf\\x81',\n",
       "  b'\\xea\\xaf\\x89\\xea\\xaf\\xa5\\xea\\xaf\\xa2\\xea\\xaf\\x94\\xea\\xaf\\x9b\\xea\\xaf\\x82\\xea\\xaf\\xa4',\n",
       "  b'\\xea\\xaf\\xab', b'[END]']                                                              ,\n",
       " [b'[START]',\n",
       "  b'\\xea\\xaf\\x87\\xea\\xaf\\xa3\\xea\\xaf\\x9d\\xea\\xaf\\x92\\xea\\xaf\\xa4',\n",
       "  b'\\xea\\xaf\\x81\\xea\\xaf\\xa5', b'##\\xea\\xaf\\x9f\\xea\\xaf\\x85',\n",
       "  b'##\\xea\\xaf\\x83\\xea\\xaf\\xa4',\n",
       "  b'##\\xea\\xaf\\x9f\\xea\\xaf\\x85\\xea\\xaf\\x85\\xea\\xaf\\x95',\n",
       "  b'\\xea\\xaf\\x83\\xea\\xaf\\x83\\xea\\xaf\\xa5\\xea\\xaf\\x9f\\xea\\xaf\\x85\\xea\\xaf\\x95',\n",
       "  b'\\xea\\xaf\\x91\\xea\\xaf\\x83\\xea\\xaf\\x87',\n",
       "  b'\\xea\\xaf\\x82\\xea\\xaf\\xa9\\xea\\xaf\\x87\\xea\\xaf\\xa6', b'\\xea\\xaf\\xab',\n",
       "  b'[END]']                                                                   ,\n",
       " [b'[START]', b'\\xea\\xaf\\x91\\xea\\xaf\\xa9\\xea\\xaf\\x85',\n",
       "  b'\\xea\\xaf\\x87\\xea\\xaf\\xa3\\xea\\xaf\\x9d',\n",
       "  b'\\xea\\xaf\\x86\\xea\\xaf\\xa0\\xea\\xaf\\x8d\\xea\\xaf\\x9f\\xea\\xaf\\x88\\xea\\xaf\\xa4',\n",
       "  b'\\xea\\xaf\\xab', b'[END]']                                                  ,\n",
       " [b'[START]', b'\\xea\\xaf\\x87\\xea\\xaf\\xa5\\xea\\xaf\\xa2',\n",
       "  b'##\\xea\\xaf\\x8c\\xea\\xaf\\x94', b'##\\xea\\xaf\\x97\\xea\\xaf\\xa8',\n",
       "  b'\\xea\\xaf\\x85\\xea\\xaf\\xa8\\xea\\xaf\\xa1\\xea\\xaf\\x81\\xea\\xaf\\xa4\\xea\\xaf\\xa0',\n",
       "  b'\\xea\\xaf\\x86\\xea\\xaf\\xa0\\xea\\xaf\\x82\\xea\\xaf\\xa4', b'\\xea\\xaf\\xab',\n",
       "  b'[END]']                                                                   ,\n",
       " [b'[START]', b'\\xea\\xaf\\x87\\xea\\xaf\\xa3\\xea\\xaf\\x9d\\xea\\xaf\\x85',\n",
       "  b'\\xea\\xaf\\x83\\xea\\xaf\\x8d\\xea\\xaf\\xa5\\xea\\xaf\\x9b',\n",
       "  b'\\xea\\xaf\\x8f\\xea\\xaf\\x85\\xea\\xaf\\xa5\\xea\\xaf\\x9b\\xea\\xaf\\x88\\xea\\xaf\\xa8\\xea\\xaf\\x9f\\xea\\xaf\\x95',\n",
       "  b'\\xea\\xaf\\x85\\xea\\xaf\\xa4\\xea\\xaf\\x84\\xea\\xaf\\xa5',\n",
       "  b'\\xea\\xaf\\x91\\xea\\xaf\\x83\\xea\\xaf\\x92\\xea\\xaf\\xa4',\n",
       "  b'\\xea\\xaf\\x83\\xea\\xaf\\x86\\xea\\xaf\\xa5\\xea\\xaf\\x85\\xea\\xaf\\xa4',\n",
       "  b'\\xea\\xaf\\x8d\\xea\\xaf\\xa5\\xea\\xaf\\xa2\\xea\\xaf\\x88\\xea\\xaf\\xa4',\n",
       "  b'\\xea\\xaf\\xab', b'[END]']                                                                          ]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('> This is the text split into tokens:')\n",
    "tokens = tokenizers.mn.lookup(encoded)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ad588d-be3c-4e14-b769-0e82d6201fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................."
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for en_examples, mn_examples in train_examples.batch(1024):\n",
    "  en_tokens = tokenizers.en.tokenize(en_examples)\n",
    "  lengths.append(en_tokens.row_lengths())\n",
    "\n",
    "  mn_tokens = tokenizers.mn.tokenize(mn_examples)\n",
    "  lengths.append(mn_tokens.row_lengths())\n",
    "  print('.', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87a4e25a-c392-4a0f-a9e3-c864b073ef22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m all_lengths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(lengths)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mhist(all_lengths, np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m101\u001b[39m))\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylim(plt\u001b[38;5;241m.\u001b[39mylim())\n\u001b[1;32m      5\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(all_lengths)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "all_lengths = np.concatenate(lengths)\n",
    "\n",
    "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
    "plt.ylim(plt.ylim())\n",
    "max_length = max(all_lengths)\n",
    "plt.plot([max_length, max_length], plt.ylim())\n",
    "plt.title(f'Maximum tokens per example: {max_length}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f7ad0-2aed-4efa-9feb-0563bdb5fb17",
   "metadata": {},
   "source": [
    "**Setting up data pipeline:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7eeabf1-e79b-48da-b2eb-b2fbf1be0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=128\n",
    "def prepare_batch(en, mn):\n",
    "    en = tokenizers.en.tokenize(en)      # Output is ragged.\n",
    "    en = en[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "    en = en.to_tensor()  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    mn = tokenizers.mn.tokenize(mn)\n",
    "    mn = mn[:, :(MAX_TOKENS+1)]\n",
    "    mn_inputs = mn[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    mn_labels = mn[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return (en, mn_inputs), mn_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2860c4af-0f9b-4e5a-b3b0-9cb78e166938",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 25000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eaac694-cf24-41dd-a103-fce263cf509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762cf0f-4237-4e0a-b23f-20338393982a",
   "metadata": {},
   "source": [
    "**Test the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b609576-83e5-4571-84d8-1868d95dcab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation set batches.\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0922801f-a7cc-4197-843e-2daf63492373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "(64, 17)\n",
      "(64, 17)\n"
     ]
    }
   ],
   "source": [
    "for (en, mn), mn_labels in train_batches.take(1):\n",
    "  break\n",
    "\n",
    "print(en.shape)\n",
    "print(mn.shape)\n",
    "print(mn_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f13f18e7-2965-4ab8-8e36-94b855fc3fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([   2  138  856 1382  264 1913   95 1365  128   80], shape=(10,), dtype=int64)\n",
      "tf.Tensor([ 138  856 1382  264 1913   95 1365  128   80    3], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(mn[0][:10])\n",
    "print(mn_labels[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6581532-97bc-4fd1-8f8a-2c2600af6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b65536e5-d6c8-4618-856a-73ece8f3ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10f76898-916c-4467-baf1-1da02b3a067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)\n",
    "embed_mn = PositionalEmbedding(vocab_size=tokenizers.mn.get_vocab_size(), d_model=512)\n",
    "\n",
    "en_emb = embed_en(en)\n",
    "mn_emb = embed_mn(mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3c8d1-5ad4-47df-93f2-6fff4cd9323c",
   "metadata": {},
   "source": [
    "**The Base Attention layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c904fc55-0e92-498d-86de-c984f48957b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f126e5e0-63b4-438c-aec1-ab30d34b9b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a5d582c-1f52-40c9-bdff-3af4601ed5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21, 512)\n",
      "(64, 17, 512)\n",
      "(64, 17, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 15:19:05.599637: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n"
     ]
    }
   ],
   "source": [
    "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(mn_emb.shape)\n",
    "print(sample_ca(mn_emb, en_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13669555-1b81-48df-b1c6-45fd1fb63d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93b4bf09-66dd-4fc2-bef2-e93baf0fbdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21, 512)\n",
      "(64, 21, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(sample_gsa(en_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64990f5e-e7c3-4647-944b-3528a6469f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5feacc5e-d3e3-40d0-979f-d1809950ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 17, 512)\n",
      "(64, 17, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(mn_emb.shape)\n",
    "print(sample_csa(mn_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a828616-2ae9-4dda-b3cb-8c00e7e48ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9604645e-07"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = sample_csa(embed_mn(mn[:, :3])) \n",
    "out2 = sample_csa(embed_mn(mn))[:, :3]\n",
    "\n",
    "tf.reduce_max(abs(out1 - out2)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c9d5e-ec8f-4c5d-8c8e-1a62a498130e",
   "metadata": {},
   "source": [
    "**The feed forward network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd5a97db-4a62-4eb5-9365-31931b37e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b06106a8-0eaa-49c4-a00b-6fd1bd4b4c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 17, 512)\n",
      "(64, 17, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_ffn = FeedForward(512, 2048)\n",
    "\n",
    "print(mn_emb.shape)\n",
    "print(sample_ffn(mn_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d93838e-18df-40ff-b1b8-6575ee95af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "417c9685-fa03-45c1-ab72-7b08e9616044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21, 512)\n",
      "(64, 21, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(sample_encoder_layer(en_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08ec8b5c-f3a4-42a8-b7d7-8e7a448b1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99898422-1dd0-414f-82fe-7efe0fd2c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "(64, 21, 512)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the encoder.\n",
    "sample_encoder = Encoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(en, training=False)\n",
    "\n",
    "# Print the shape.\n",
    "print(en.shape)\n",
    "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673a0ea-4d29-4c34-8d09-bbec0429cb93",
   "metadata": {},
   "source": [
    "**The decoder layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b214e8d7-1d05-41d9-b89d-8dcb5b3cf849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0f353c0-2cd5-48b9-92fe-9fce93f2a9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 17, 512)\n",
      "(64, 21, 512)\n",
      "(64, 17, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_decoder_layer_output = sample_decoder_layer(\n",
    "    x=mn_emb, context=en_emb)\n",
    "\n",
    "print(mn_emb.shape)\n",
    "print(en_emb.shape)\n",
    "print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c23e4f-a7f7-4196-b7e5-fc027f9fb98e",
   "metadata": {},
   "source": [
    "**The decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "015b5e06-637e-4f86-973a-e176aaa64b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb517d6a-93cf-4c13-a396-62a3ce4b680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 17)\n",
      "(64, 21, 512)\n",
      "(64, 17, 512)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=8000)\n",
    "\n",
    "output = sample_decoder(\n",
    "    x=mn,\n",
    "    context=en_emb)\n",
    "\n",
    "# Print the shapes.\n",
    "print(mn.shape)\n",
    "print(en_emb.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba634213-c7be-4261-afc7-92fdc49554f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 8, 17, 21])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "430b8151-d98e-4d28-8243-c45e56fe4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aee710-5b00-4c47-9395-aafc7be331a9",
   "metadata": {},
   "source": [
    "**Hyperparameters**\n",
    "\n",
    "The base model described in the original Transformer paper used num_layers=6, d_model=512, and dff=2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "776ec3ab-b133-4076-b61b-4a73cc58f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.2 #original dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ee9b764-5948-4a9e-be63-b0d46bc2d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.mn.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a62456d-ba11-475b-80a1-2dc2263b2143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 17)\n",
      "(64, 21)\n",
      "(64, 17, 3526)\n"
     ]
    }
   ],
   "source": [
    "output = transformer((en, mn))\n",
    "\n",
    "print(mn.shape)\n",
    "print(en.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d793a7af-f3e0-470e-94c1-300d26e9797e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 8, 17, 21)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
    "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a166c63a-ab59-4f23-9d63-9d7d41001569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_3 (Encoder)         multiple                  2947584   \n",
      "                                                                 \n",
      " decoder_3 (Decoder)         multiple                  5201152   \n",
      "                                                                 \n",
      " dense_72 (Dense)            multiple                  454854    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8603590 (32.82 MB)\n",
      "Trainable params: 8603590 (32.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2204d2b7-f680-4f44-82cf-5491b444a943",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b1b91f2-29eb-4b52-8f4d-f952bb8b0076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous warmup_steps = 4000\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d93f785-464b-4aff-80ac-6a50daa1613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbd65783-6a37-4f10-80ec-72ea5834425c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(learning_rate(tf\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m40000\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning Rate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2656331c-ecda-4086-8393-5d85003db8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fbaf2-58ce-414c-a70e-9c99c0eccd15",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "523eb311-83bf-4d1b-b28d-8637b5696e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bbed2c00-03a0-4c93-8dee-f7dc83ca4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 15:27:57.793996: I external/local_xla/xla/service/service.cc:168] XLA service 0x7efd80009450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-12 15:27:57.794016: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2070, Compute Capability 7.5\n",
      "2024-01-12 15:27:57.957271: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1705053478.316632   16587 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830/830 [==============================] - 68s 60ms/step - loss: 5.7230 - masked_accuracy: 0.2842 - val_loss: 4.2481 - val_masked_accuracy: 0.3749\n",
      "Epoch 2/20\n",
      "830/830 [==============================] - 40s 49ms/step - loss: 3.7480 - masked_accuracy: 0.4224 - val_loss: 3.2495 - val_masked_accuracy: 0.4749\n",
      "Epoch 3/20\n",
      "830/830 [==============================] - 40s 48ms/step - loss: 2.9417 - masked_accuracy: 0.4994 - val_loss: 2.5625 - val_masked_accuracy: 0.5436\n",
      "Epoch 4/20\n",
      "830/830 [==============================] - 41s 49ms/step - loss: 2.4015 - masked_accuracy: 0.5553 - val_loss: 2.1985 - val_masked_accuracy: 0.5829\n",
      "Epoch 5/20\n",
      "830/830 [==============================] - 40s 49ms/step - loss: 2.0981 - masked_accuracy: 0.5870 - val_loss: 2.0025 - val_masked_accuracy: 0.6101\n",
      "Epoch 6/20\n",
      "830/830 [==============================] - 40s 49ms/step - loss: 1.8634 - masked_accuracy: 0.6185 - val_loss: 1.8277 - val_masked_accuracy: 0.6356\n",
      "Epoch 7/20\n",
      "830/830 [==============================] - 40s 49ms/step - loss: 1.6546 - masked_accuracy: 0.6475 - val_loss: 1.7255 - val_masked_accuracy: 0.6533\n",
      "Epoch 8/20\n",
      "830/830 [==============================] - 40s 48ms/step - loss: 1.4984 - masked_accuracy: 0.6709 - val_loss: 1.6549 - val_masked_accuracy: 0.6650\n",
      "Epoch 9/20\n",
      "830/830 [==============================] - 40s 49ms/step - loss: 1.3732 - masked_accuracy: 0.6908 - val_loss: 1.5866 - val_masked_accuracy: 0.6772\n",
      "Epoch 10/20\n",
      "830/830 [==============================] - 41s 49ms/step - loss: 1.2761 - masked_accuracy: 0.7065 - val_loss: 1.5369 - val_masked_accuracy: 0.6856\n",
      "Epoch 11/20\n",
      "830/830 [==============================] - 40s 49ms/step - loss: 1.1919 - masked_accuracy: 0.7214 - val_loss: 1.5262 - val_masked_accuracy: 0.6955\n",
      "Epoch 12/20\n",
      "830/830 [==============================] - 40s 48ms/step - loss: 1.1197 - masked_accuracy: 0.7331 - val_loss: 1.5101 - val_masked_accuracy: 0.6988\n",
      "Epoch 13/20\n",
      "830/830 [==============================] - 41s 49ms/step - loss: 1.0547 - masked_accuracy: 0.7443 - val_loss: 1.5007 - val_masked_accuracy: 0.7010\n",
      "Epoch 14/20\n",
      "830/830 [==============================] - 41s 49ms/step - loss: 1.0036 - masked_accuracy: 0.7540 - val_loss: 1.4778 - val_masked_accuracy: 0.7068\n",
      "Epoch 15/20\n",
      "830/830 [==============================] - 41s 49ms/step - loss: 0.9500 - masked_accuracy: 0.7626 - val_loss: 1.4692 - val_masked_accuracy: 0.7125\n",
      "Epoch 16/20\n",
      "830/830 [==============================] - 41s 49ms/step - loss: 0.9058 - masked_accuracy: 0.7711 - val_loss: 1.4782 - val_masked_accuracy: 0.7104\n",
      "Epoch 17/20\n",
      "830/830 [==============================] - 41s 49ms/step - loss: 0.8666 - masked_accuracy: 0.7791 - val_loss: 1.4788 - val_masked_accuracy: 0.7146\n",
      "Epoch 18/20\n",
      "830/830 [==============================] - 40s 48ms/step - loss: 0.8285 - masked_accuracy: 0.7859 - val_loss: 1.4586 - val_masked_accuracy: 0.7189\n",
      "Epoch 19/20\n",
      "830/830 [==============================] - 41s 49ms/step - loss: 0.7927 - masked_accuracy: 0.7928 - val_loss: 1.4687 - val_masked_accuracy: 0.7223\n",
      "Epoch 20/20\n",
      "830/830 [==============================] - 40s 49ms/step - loss: 0.7649 - masked_accuracy: 0.7983 - val_loss: 1.4623 - val_masked_accuracy: 0.7228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7efe6bc9d1e0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_batches,\n",
    "                epochs=20,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6024e5f-e19a-4998-ba33-9c09588bc8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  def __init__(self, tokenizers, transformer):\n",
    "    self.tokenizers = tokenizers\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "    # The input sentence is English, hence adding the `[START]` and `[END]` tokens.\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    sentence = self.tokenizers.en.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # As the output language is Manipuri, initialize the output with the\n",
    "    # English `[START]` token.\n",
    "    start_end = self.tokenizers.mn.tokenize([''])[0]\n",
    "    start = start_end[0][tf.newaxis]\n",
    "    end = start_end[1][tf.newaxis]\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # The output shape is `(1, tokens)`.\n",
    "    text = tokenizers.mn.detokenize(output)[0]  # Shape: `()`.\n",
    "    #result = text.numpy().decode(\"utf-8\")\n",
    "    # Commenting out the below line to see if tflite works\n",
    "    #tokens = tokenizers.mn.lookup(output)[0]\n",
    "    tokens = \"\"\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08ab241b-75ab-468f-9819-4ddded528f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba23ad9d-db16-414f-bb62-f00a6e4b87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25478eea-d1d5-46b3-8a99-06b5a29238ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : We have met once.\n",
      "Prediction     : ꯑꯩꯈꯣꯢ ꯑꯃꯨꯛꯈꯛ ꯎꯅꯈꯤ ꯫\n",
      "Ground truth   : \n"
     ]
    }
   ],
   "source": [
    "sentence = \"We have met once.\"\n",
    "ground_truth = ''\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d809021-4aca-477b-8060-bc942d2d9f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : He said that 'he love his friend's wife'.\n",
      "Prediction     : ꯃꯍꯥꯛꯅ ꯍꯥꯢꯈꯤ ꯃꯗꯨꯗꯤ ꯃꯍꯥꯛꯀꯤ ꯃꯔꯨꯞꯅꯤ ꯫\n",
      "Ground truth   : ꯃꯍꯥꯛ ꯃꯨꯆꯤꯡꯕꯒꯤ ꯂꯨꯆꯤꯡꯕꯒꯤ ꯂꯝꯖꯉꯥꯢꯐꯗꯅ ꯈꯪꯉꯤ ꯫\n"
     ]
    }
   ],
   "source": [
    "sentence = \"He said that 'he love his friend\\'s wife'.\"\n",
    "ground_truth = 'ꯃꯍꯥꯛ ꯃꯨꯆꯤꯡꯕꯒꯤ ꯂꯨꯆꯤꯡꯕꯒꯤ ꯂꯝꯖꯉꯥꯢꯐꯗꯅ ꯈꯪꯉꯤ ꯫'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6247d122-fa90-493f-8f79-6cc82eba73b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
    "  # The model didn't generate `<START>` in the output. Skip it.\n",
    "  translated_tokens = translated_tokens[1:]\n",
    "\n",
    "  ax = plt.gca()\n",
    "  ax.matshow(attention)\n",
    "  ax.set_xticks(range(len(in_tokens)))\n",
    "  ax.set_yticks(range(len(translated_tokens)))\n",
    "\n",
    "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
    "  ax.set_xticklabels(\n",
    "      labels, rotation=90)\n",
    "\n",
    "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
    "  ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e185135-f810-4181-8589-5863bce082f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 9])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = 0\n",
    "# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.\n",
    "attention_heads = tf.squeeze(attention_weights, 0)\n",
    "attention = attention_heads[head]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90761bb9-c84f-4c3e-b806-a14f997f835f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=string, numpy=\n",
       "array([b'[START]', b'no', b'one', b'knew', b'who', b'he', b'was', b'.',\n",
       "       b'[END]'], dtype=object)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_tokens = tf.convert_to_tensor([sentence])\n",
    "in_tokens = tokenizers.en.tokenize(in_tokens).to_tensor()\n",
    "in_tokens = tokenizers.en.lookup(in_tokens)[0]\n",
    "in_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e0592888-4ba8-4313-82df-58c545562fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=string, numpy=\n",
       "array([b'[START]',\n",
       "       b'\\xea\\xaf\\x80\\xea\\xaf\\x85\\xea\\xaf\\xa5\\xea\\xaf\\x92\\xea\\xaf\\xa8\\xea\\xaf\\x9d\\xea\\xaf\\x95',\n",
       "       b'\\xea\\xaf\\x91\\xea\\xaf\\x83\\xea\\xaf\\x87',\n",
       "       b'\\xea\\xaf\\x83\\xea\\xaf\\x8d\\xea\\xaf\\xa5\\xea\\xaf\\x9b',\n",
       "       b'\\xea\\xaf\\x80\\xea\\xaf\\x85\\xea\\xaf\\xa5\\xea\\xaf\\x85\\xea\\xaf\\xa3',\n",
       "       b'\\xea\\xaf\\x8d\\xea\\xaf\\xa5\\xea\\xaf\\xa2\\xea\\xaf\\x95',\n",
       "       b'\\xea\\xaf\\x88\\xea\\xaf\\xaa\\xea\\xaf\\x88\\xea\\xaf\\xa4\\xea\\xaf\\x97\\xea\\xaf\\xa6',\n",
       "       b'\\xea\\xaf\\xab', b'[END]'], dtype=object)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "124bd29d-7834-49cf-b024-ebe95fae665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_attention_head(in_tokens, translated_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e66b1f5b-678d-4029-94b9-a6a747325b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
    "  in_tokens = tf.convert_to_tensor([sentence])\n",
    "  in_tokens = tokenizers.en.tokenize(in_tokens).to_tensor()\n",
    "  in_tokens = tokenizers.en.lookup(in_tokens)[0]\n",
    "\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "  for h, head in enumerate(attention_heads):\n",
    "    ax = fig.add_subplot(2, 4, h+1)\n",
    "\n",
    "    plot_attention_head(in_tokens, translated_tokens, head)\n",
    "\n",
    "    ax.set_xlabel(f'Head {h+1}')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "032dc820-99d9-43dd-8e93-714828718171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 43975 (\\N{MEETEI MAYEK LETTER TIL}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 44003 (\\N{MEETEI MAYEK VOWEL SIGN ONAP}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 43997 (\\N{MEETEI MAYEK LETTER MIT LONSUM}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 43991 (\\N{MEETEI MAYEK LETTER DIL}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 43971 (\\N{MEETEI MAYEK LETTER MIT}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 44008 (\\N{MEETEI MAYEK VOWEL SIGN UNAP}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 44007 (\\N{MEETEI MAYEK VOWEL SIGN SOUNAP}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 43973 (\\N{MEETEI MAYEK LETTER NA}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 43989 (\\N{MEETEI MAYEK LETTER BA}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 43981 (\\N{MEETEI MAYEK LETTER HUK}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 44010 (\\N{MEETEI MAYEK VOWEL SIGN NUNG}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 43976 (\\N{MEETEI MAYEK LETTER KHOU}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 44004 (\\N{MEETEI MAYEK VOWEL SIGN INAP}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 43985 (\\N{MEETEI MAYEK LETTER ATIYA}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 44009 (\\N{MEETEI MAYEK VOWEL SIGN CHEINAP}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_5987/4046938539.py:15: UserWarning: Glyph 44011 (\\N{MEETEI MAYEK CHEIKHEI}) missing from current font.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_attention_weights(sentence,\n",
    "                       translated_tokens,\n",
    "                       attention_weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433faf76-5960-4530-a175-5b17927c6ef0",
   "metadata": {},
   "source": [
    "**Export the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "39e8c692-7219-435e-8759-c155d98407e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "184004b6-a8a2-4e22-8881-e799967d874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "  def __init__(self, translator):\n",
    "    self.translator = translator\n",
    "\n",
    "  #@tf.function(input_signature=[tf.TensorSpec([], tf.float32)])\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[1,], dtype=tf.string)])\n",
    "  def __call__(self, sentence) -> Dict[str, str]:\n",
    "    (result, tokens, attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
    "\n",
    "    return {\"prediction\": tf.strings.as_string([result])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5df98b3a-a004-4c57-be95-62c5b54eed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translate_manipuri_model/gpu-4l-20epoch/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translate_manipuri_model/gpu-4l-20epoch/assets\n"
     ]
    }
   ],
   "source": [
    "model_output = ExportTranslator(translator)\n",
    "call_output = model_output.__call__.get_concrete_function(tf.TensorSpec(shape=[1,], dtype=tf.string))\n",
    "translator_model_name = \"translate_manipuri_model/gpu-4l-20epoch\"\n",
    "tf.saved_model.save(model_output, export_dir = translator_model_name, signatures={'serving_default': call_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d8bfe9ba-45c5-40ff-80a4-4c072ead607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ExportTranslator(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8bb3c3a6-ab4f-4ba1-9ce7-2f2933575e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ꯑꯍꯦꯟꯕ ꯁꯒꯣꯜꯁꯤꯡ ꯅꯤꯡꯖꯕ ꯀꯥꯢꯈꯤ ꯫'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = translator([\"More and more Kuki militants lost hope.\"])\n",
    "(d[\"prediction\"][0]).numpy().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0aed50-5e8d-4670-bd4b-ce4165aa0a9b",
   "metadata": {},
   "source": [
    "**Lets try to save the TF lite model here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "57471784-f4c2-440d-942c-767c01bae8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as tf_text\n",
    "#import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f6ded6f-9b27-47fd-b680-f153e18d20f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp76y3kpgw/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp76y3kpgw/assets\n",
      "2024-01-12 16:19:39.953563: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-12 16:19:39.953583: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-12 16:19:39.953839: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp76y3kpgw\n",
      "2024-01-12 16:19:39.980366: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-12 16:19:39.980377: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp76y3kpgw\n",
      "2024-01-12 16:19:40.041811: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-01-12 16:19:40.063379: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-12 16:19:40.801941: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp76y3kpgw\n",
      "2024-01-12 16:19:41.128386: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1174548 microseconds.\n",
      "2024-01-12 16:19:44.082464: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082485: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082494: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082501: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082508: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082515: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082522: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082528: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082534: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082542: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082548: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082555: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082568: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082574: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082578: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082582: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082585: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082590: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082593: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082597: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082616: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082621: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082633: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082640: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082644: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082650: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082656: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082662: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082667: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082675: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082680: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082684: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082688: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082691: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082695: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082701: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082709: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082727: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082731: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082735: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082739: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082743: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-01-12 16:19:44.082956: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 301, Total Ops 1793, % non-converted = 16.79 %\n",
      " * 250 ARITH ops, 47 TF ops, 4 TF_SAVED_MODEL ops\n",
      "\n",
      "- arith.constant:  250 occurrences  (: 5, i64: 11, f32: 179, i1: 5, i32: 50)\n",
      "\n",
      "  (i1: 1, i32: 1)\n",
      "\n",
      "\n",
      "- tf.AsString:    1 occurrences  (: 1)\n",
      "- tf.CaseFoldUTF8:    1 occurrences  (: 1)\n",
      "- tf.ConcatV2:    1 occurrences  (: 1)\n",
      "- tf.DenseBincount:    1 occurrences  (i64: 1)\n",
      "- tf.EnsureShape:    2 occurrences  (: 1, i64: 1)\n",
      "- tf.HashTableV2:    2 occurrences  (: 2)\n",
      "- tf.If:    2 occurrences  (i1: 2)\n",
      "- tf.LookupTableExportV2:    1 occurrences  (: 1)\n",
      "- tf.LookupTableFindV2:    2 occurrences  (i64: 2)\n",
      "- tf.LookupTableSizeV2:    2 occurrences  (i64: 2)\n",
      "- tf.NormalizeUTF8:    1 occurrences  (: 1)\n",
      "- tf.RaggedGather:    2 occurrences  (i64: 2)\n",
      "- tf.RaggedTensorToTensor:    1 occurrences  (i64: 1)\n",
      "- tf.RegexSplitWithOffsets:    1 occurrences  (: 1)\n",
      "- tf.StaticRegexFullMatch:    1 occurrences  (i1: 1)\n",
      "- tf.StaticRegexReplace:    4 occurrences  (: 4)\n",
      "- tf.StridedSlice:    9 occurrences  (i1: 9)\n",
      "- tf.StringSplitV2:    1 occurrences  (i64: 1)\n",
      "- tf.StringToHashBucketFast:    2 occurrences  (i64: 2)\n",
      "- tf.TensorListReserve:    1 occurrences  (: 1)\n",
      "- tf.TensorListSetItem:    2 occurrences  (: 2)\n",
      "- tf.TensorListStack:    2 occurrences  (i64: 2)\n",
      "- tf.UnsortedSegmentJoin:    2 occurrences  (: 2)\n",
      "- tf.UnsortedSegmentSum:    1 occurrences  (i64: 1)\n",
      "- tf.WordpieceTokenizeWithOffsets:    2 occurrences  (: 2)\n",
      "- tf_saved_model.asset:    4 occurrences\n",
      "  (i64: 10, f32: 147, i32: 3)\n",
      "  (i64: 1)\n",
      "  (f32: 24)\n",
      "  (i1: 4)\n",
      "  (i64: 2, f32: 12, i32: 11)\n",
      "  (i64: 8, i1: 18, i32: 28)\n",
      "  (i64: 2)\n",
      "  (: 11, i64: 3)\n",
      "  (i1: 1)\n",
      "  (i64: 2, i1: 30, i32: 6)\n",
      "  (i64: 4, f32: 40, i1: 4, i32: 2)\n",
      "  (f32: 65)\n",
      "  (: 3, i64: 5, f32: 2, i32: 45)\n",
      "  (i1: 1)\n",
      "  (i1: 4)\n",
      "  (i1: 4)\n",
      "  (i1: 4)\n",
      "  (i1: 32)\n",
      "  (i1: 2)\n",
      "  (i64: 2, i32: 4)\n",
      "  (f32: 40)\n",
      "  (i64: 1, i32: 1)\n",
      "  (i64: 3, f32: 106, i32: 3)\n",
      "  (i64: 1, i32: 1)\n",
      "\n",
      "  (i1: 4)\n",
      "  (i64: 4, i32: 46)\n",
      "  (i64: 6, i32: 11)\n",
      "  (i1: 18)\n",
      "  (i64: 2, i32: 4)\n",
      "  (i32: 59)\n",
      "  (: 3, i64: 22, f32: 124, i1: 17, i32: 24)\n",
      "  (f32: 20)\n",
      "  (i64: 2, i1: 4)\n",
      "  (i64: 10, i32: 77)\n",
      "  (f32: 12)\n",
      "  (f32: 20)\n",
      "  (: 1, i64: 41, f32: 3, i1: 12, i32: 39)\n",
      "  (i64: 9, f32: 32, i32: 6)\n",
      "  (i64: 3)\n",
      "  (i64: 1)\n",
      "  (i64: 4, f32: 100)\n",
      "  (i32: 11)\n",
      "  (i64: 4)\n",
      "  (i32: 1)\n",
      "\n",
      "2024-01-12 16:19:44.163691: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2910] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:\n",
      "Resource ops: HashTableV2, LookupTableExportV2, LookupTableFindV2, LookupTableSizeV2, WordpieceTokenizeWithOffsets\n",
      "Details:\n",
      "\ttf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", key_dtype = !tf_type.string, shared_name = \"hash_table_en_vocab.txt_-2_-1_load_18_1611\", use_node_name_sharing = true, value_dtype = i64}\n",
      "\ttf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", key_dtype = !tf_type.string, shared_name = \"hash_table_mn_vocab.txt_-2_-1_load_18_1190\", use_node_name_sharing = true, value_dtype = i64}\n",
      "\ttf.LookupTableExportV2(tensor<!tf_type.resource>) -> (tensor<*x!tf_type.string>, tensor<*xi64>) : {device = \"\"}\n",
      "\ttf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<?x!tf_type.string>, tensor<i64>) -> (tensor<*xi64>) : {device = \"\"}\n",
      "\ttf.LookupTableSizeV2(tensor<!tf_type.resource>) -> (tensor<i64>) : {device = \"\"}\n",
      "\ttf.WordpieceTokenizeWithOffsets(tensor<0x!tf_type.string>, tensor<!tf_type.resource>) -> (tensor<?x!tf_type.string>, tensor<1xi64>, tensor<?xi64>, tensor<?xi64>) : {device = \"\", max_bytes_per_word = 100 : i64, max_chars_per_token = 0 : i64, output_row_partition_type = \"row_splits\", split_unknown_characters = false, suffix_indicator = \"##\", unknown_token = \"[UNK]\", use_unknown_token = true}\n",
      "\ttf.WordpieceTokenizeWithOffsets(tensor<?x!tf_type.string>, tensor<!tf_type.resource>) -> (tensor<?x!tf_type.string>, tensor<?xi64>, tensor<?xi64>, tensor<?xi64>) : {device = \"\", max_bytes_per_word = 100 : i64, max_chars_per_token = 0 : i64, output_row_partition_type = \"row_splits\", split_unknown_characters = false, suffix_indicator = \"##\", unknown_token = \"[UNK]\", use_unknown_token = true}\n",
      "2024-01-12 16:19:44.163723: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2921] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexAsString, FlexCaseFoldUTF8, FlexConcatV2, FlexDenseBincount, FlexEnsureShape, FlexHashTableV2, FlexLookupTableExportV2, FlexLookupTableFindV2, FlexLookupTableSizeV2, FlexNormalizeUTF8, FlexRaggedGather, FlexRaggedTensorToTensor, FlexRegexSplitWithOffsets, FlexStaticRegexFullMatch, FlexStaticRegexReplace, FlexStridedSlice, FlexStringSplitV2, FlexStringToHashBucketFast, FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack, FlexUnsortedSegmentJoin, FlexUnsortedSegmentSum, FlexWordpieceTokenizeWithOffsets\n",
      "Details:\n",
      "\ttf.AsString(tensor<1x!tf_type.string>) -> (tensor<1x!tf_type.string>) : {device = \"\", fill = \"\", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}\n",
      "\ttf.CaseFoldUTF8(tensor<1x!tf_type.string>) -> (tensor<1x!tf_type.string>) : {device = \"\"}\n",
      "\ttf.ConcatV2(tensor<?x!tf_type.string>, tensor<1x!tf_type.string>, tensor<i32>) -> (tensor<?x!tf_type.string>) : {device = \"\"}\n",
      "\ttf.DenseBincount(tensor<?xi32>, tensor<i32>, tensor<0xi64>) -> (tensor<?xi64>) : {T = i64, Tidx = i32, binary_output = false, device = \"\"}\n",
      "\ttf.EnsureShape(tensor<*x!tf_type.string>) -> (tensor<?x!tf_type.string>) : {device = \"\", shape = #tf_type.shape<?>}\n",
      "\ttf.EnsureShape(tensor<*xi64>) -> (tensor<?xi64>) : {device = \"\", shape = #tf_type.shape<?>}\n",
      "\ttf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", key_dtype = !tf_type.string, shared_name = \"hash_table_en_vocab.txt_-2_-1_load_18_1611\", use_node_name_sharing = true, value_dtype = i64}\n",
      "\ttf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", key_dtype = !tf_type.string, shared_name = \"hash_table_mn_vocab.txt_-2_-1_load_18_1190\", use_node_name_sharing = true, value_dtype = i64}\n",
      "\ttf.LookupTableExportV2(tensor<!tf_type.resource>) -> (tensor<*x!tf_type.string>, tensor<*xi64>) : {device = \"\"}\n",
      "\ttf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<?x!tf_type.string>, tensor<i64>) -> (tensor<*xi64>) : {device = \"\"}\n",
      "\ttf.LookupTableSizeV2(tensor<!tf_type.resource>) -> (tensor<i64>) : {device = \"\"}\n",
      "\ttf.NormalizeUTF8(tensor<1x!tf_type.string>) -> (tensor<1x!tf_type.string>) : {device = \"\", normalization_form = \"NFD\"}\n",
      "\ttf.RaggedGather(tensor<?xi64>, tensor<?xi64>, tensor<?xi64>) -> (tensor<?xi64>, tensor<?xi64>) : {device = \"\"}\n",
      "\ttf.RaggedTensorToTensor(tensor<i64>, tensor<?xi64>, tensor<i64>, tensor<?xi64>) -> (tensor<?x?xi64>) : {T = i64, Tindex = i64, Tshape = i64, device = \"\", num_row_partition_tensors = 1 : i64, row_partition_types = [\"ROW_SPLITS\"]}\n",
      "\ttf.RegexSplitWithOffsets(tensor<1x!tf_type.string>, tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<?x!tf_type.string>, tensor<?xi64>, tensor<?xi64>, tensor<?xi64>) : {device = \"\"}\n",
      "\ttf.StaticRegexFullMatch(tensor<?x!tf_type.string>) -> (tensor<?xi1>) : {device = \"\", pattern = \"\\\\[PAD\\\\]|\\\\[START\\\\]|\\\\[END\\\\]\"}\n",
      "\ttf.StaticRegexReplace(tensor<1x!tf_type.string>) -> (tensor<1x!tf_type.string>) : {device = \"\", pattern = \" \\\\#\\\\#\", replace_global = true, rewrite = \"\"}\n",
      "\ttf.StaticRegexReplace(tensor<1x!tf_type.string>) -> (tensor<1x!tf_type.string>) : {device = \"\", pattern = \"\\\\p{Cc}|\\\\p{Cf}\", replace_global = true, rewrite = \" \"}\n",
      "\ttf.StaticRegexReplace(tensor<1x!tf_type.string>) -> (tensor<1x!tf_type.string>) : {device = \"\", pattern = \"\\\\p{Mn}\", replace_global = true, rewrite = \"\"}\n",
      "\ttf.StaticRegexReplace(tensor<1x!tf_type.string>) -> (tensor<1x!tf_type.string>) : {device = \"\", pattern = \"^ +| +$\", replace_global = true, rewrite = \"\"}\n",
      "\ttf.StridedSlice(tensor<?x?xi1>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<?x1x?xi1>) : {begin_mask = 5 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 5 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}\n",
      "\ttf.StridedSlice(tensor<?x?xi1>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<?x?x1xi1>) : {begin_mask = 3 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 3 : i64, new_axis_mask = 4 : i64, shrink_axis_mask = 0 : i64}\n",
      "\ttf.StringSplitV2(tensor<1x!tf_type.string>, tensor<!tf_type.string>) -> (tensor<?x2xi64>, tensor<?x!tf_type.string>, tensor<2xi64>) : {device = \"\", maxsplit = -1 : i64}\n",
      "\ttf.StringToHashBucketFast(tensor<?x!tf_type.string>) -> (tensor<?xi64>) : {device = \"\", num_buckets = 1 : i64}\n",
      "\ttf.TensorListReserve(tensor<i32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<*xi64>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<*xi64>>>, tensor<i32>, tensor<1xi64>) -> (tensor<!tf_type.variant<tensor<*xi64>>>) : {device = \"\", resize_if_index_out_of_bounds = true}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<*xi64>>>, tensor<i32>, tensor<?xi64>) -> (tensor<!tf_type.variant<tensor<*xi64>>>) : {device = \"\", resize_if_index_out_of_bounds = true}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<*xi64>>>, tensor<1xi32>) -> (tensor<?x1xi64>) : {device = \"\", num_elements = -1 : i64}\n",
      "\ttf.UnsortedSegmentJoin(tensor<?x!tf_type.string>, tensor<?xi64>, tensor<i32>) -> (tensor<1x!tf_type.string>) : {Tindices = i64, Tnumsegments = i32, device = \"\", separator = \" \"}\n",
      "\ttf.UnsortedSegmentJoin(tensor<?x!tf_type.string>, tensor<?xi64>, tensor<i32>) -> (tensor<?x!tf_type.string>) : {Tindices = i64, Tnumsegments = i32, device = \"\", separator = \" \"}\n",
      "\ttf.UnsortedSegmentSum(tensor<?xi64>, tensor<?xi64>, tensor<i32>) -> (tensor<?xi64>) : {device = \"\"}\n",
      "\ttf.WordpieceTokenizeWithOffsets(tensor<0x!tf_type.string>, tensor<!tf_type.resource>) -> (tensor<?x!tf_type.string>, tensor<1xi64>, tensor<?xi64>, tensor<?xi64>) : {device = \"\", max_bytes_per_word = 100 : i64, max_chars_per_token = 0 : i64, output_row_partition_type = \"row_splits\", split_unknown_characters = false, suffix_indicator = \"##\", unknown_token = \"[UNK]\", use_unknown_token = true}\n",
      "\ttf.WordpieceTokenizeWithOffsets(tensor<?x!tf_type.string>, tensor<!tf_type.resource>) -> (tensor<?x!tf_type.string>, tensor<?xi64>, tensor<?xi64>, tensor<?xi64>) : {device = \"\", max_bytes_per_word = 100 : i64, max_chars_per_token = 0 : i64, output_row_partition_type = \"row_splits\", split_unknown_characters = false, suffix_indicator = \"##\", unknown_token = \"[UNK]\", use_unknown_token = true}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    }
   ],
   "source": [
    "# Convert the model.\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(translator)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "#converter._experimental_lower_tensor_list_ops = False\n",
    "tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4637fbf4-a8e7-404c-b0d7-79f3b222c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "35ac4891-b4e4-4fd5-881f-649a2cbbf308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, positional_embedding_4_layer_call_fn, positional_embedding_4_layer_call_and_return_conditional_losses, dropout_35_layer_call_fn, dropout_35_layer_call_and_return_conditional_losses while saving (showing 5 of 317). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translate_manipuri_model/2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translate_manipuri_model/2/assets\n"
     ]
    }
   ],
   "source": [
    "translator_model_name = \"translate_manipuri_model/2\"\n",
    "tf.saved_model.save(translator, export_dir = translator_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff5f4b-cfc3-4614-9d5b-260525e2b1fe",
   "metadata": {},
   "source": [
    "**Save a TF lite model as well **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9600f1f8-21c3-44c0-89a2-99b52a98172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_model_name = \"translate_manipuri_model/2\"\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(translator_model_name)\n",
    "converter.target_spec.supported_ops = [ tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "42d7756f-9f71-41de-9976-249acd49eedc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'call'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m converter\u001b[38;5;241m.\u001b[39mexperimental_new_converter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save the model.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtm.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/media2/translateManipuriInstallation/miniconda3/envs/homl3/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:933\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    932\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media2/translateManipuriInstallation/miniconda3/envs/homl3/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:911\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m    910\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m--> 911\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[0;32m/media2/translateManipuriInstallation/miniconda3/envs/homl3/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1347\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1344\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n\u001b[1;32m   1346\u001b[0m graph_def, input_tensors, output_tensors, frozen_func \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_freeze_keras_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1349\u001b[0m graph_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_tf_model(graph_def, input_tensors,\n\u001b[1;32m   1350\u001b[0m                                     output_tensors, frozen_func)\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(TFLiteKerasModelConverterV2,\n\u001b[1;32m   1353\u001b[0m              \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(graph_def, input_tensors, output_tensors)\n",
      "File \u001b[0;32m/media2/translateManipuriInstallation/miniconda3/envs/homl3/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media2/translateManipuriInstallation/miniconda3/envs/homl3/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m/media2/translateManipuriInstallation/miniconda3/envs/homl3/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1290\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._freeze_keras_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1284\u001b[0m input_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# If the model's call is not a `tf.function`, then we need to first get its\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# input signature from `model_input_signature` method. We can't directly\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m# call `trace_model_call` because otherwise the batch dimension is set\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# to None.\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;66;03m# Once we have better support for dynamic shapes, we can remove this.\u001b[39;00m\n\u001b[0;32m-> 1290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m, _def_function\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[1;32m   1291\u001b[0m   \u001b[38;5;66;03m# Pass `keep_original_batch_size=True` will ensure that we get an input\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m   \u001b[38;5;66;03m# signature including the batch dimension specified by the user.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m   \u001b[38;5;66;03m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m   input_signature \u001b[38;5;241m=\u001b[39m _model_input_signature(\n\u001b[1;32m   1295\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_model, keep_original_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'call'"
     ]
    }
   ],
   "source": [
    "converter.experimental_new_converter = True\n",
    "tflite_model = converter.convert()\n",
    "# Save the model.\n",
    "model_name = \"tm.tflite\"\n",
    "with open(model_name, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50f9ad3a-db86-469c-9871-d549241771e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load(translator_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63d6ae30-7b5f-467c-bcf9-e0f1585abd73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Python inputs incompatible with input_signature:\n  inputs: (\n    I am not your teacher.)\n  input_signature: (\n    TensorSpec(shape=(1,), dtype=tf.string, name=None)).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreloaded\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mI am not your teacher.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/media2/translateManipuriInstallation/miniconda3/envs/homl3/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:732\u001b[0m, in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_attribute\u001b[39m(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 732\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media2/translateManipuriInstallation/miniconda3/envs/homl3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/media2/translateManipuriInstallation/miniconda3/envs/homl3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:698\u001b[0m, in \u001b[0;36mconvert_inputs_to_signature\u001b[0;34m(inputs, input_signature, flat_input_signature)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen input_signature is provided, all inputs to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    691\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe Python function must be convertible to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    692\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    693\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_error_message(inputs,\u001b[38;5;250m \u001b[39minput_signature)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mis_compatible_with(other) \u001b[38;5;28;01mfor\u001b[39;00m spec, other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    696\u001b[0m     flat_input_signature,\n\u001b[1;32m    697\u001b[0m     flatten_inputs)):\n\u001b[0;32m--> 698\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython inputs incompatible with input_signature:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    699\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_error_message(inputs,\u001b[38;5;250m \u001b[39minput_signature)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_packing:\n\u001b[1;32m    702\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m    703\u001b[0m       structure\u001b[38;5;241m=\u001b[39minput_signature,\n\u001b[1;32m    704\u001b[0m       flat_sequence\u001b[38;5;241m=\u001b[39mflatten_inputs,\n\u001b[1;32m    705\u001b[0m       expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Python inputs incompatible with input_signature:\n  inputs: (\n    I am not your teacher.)\n  input_signature: (\n    TensorSpec(shape=(1,), dtype=tf.string, name=None))."
     ]
    }
   ],
   "source": [
    "reloaded('I am not your teacher.').numpy().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c313ec49-8b67-4943-93ea-178e82d0dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from shutil import copy\n",
    "#from translate import translate_bengali_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6a52709-fa4e-4da6-a0cd-c96257e64498",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"/home/surchand/projects/TranslateManipuri/processing/test.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea4eb7ae-b3c0-4c15-91a0-3f5766f413d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(file_location):\n",
    "    #reloaded = tf.saved_model.load(translator_model_name)\n",
    "    readb = xlrd.open_workbook(file_location)\n",
    "    writeb = readb\n",
    "    readsheet = readb.sheet_by_index(0)\n",
    "    writesheet = writeb.get_sheet(0)\n",
    "    nrows = readsheet.nrows\n",
    "    # skip the first rows as it is header\n",
    "    for i in range(1, nrows):\n",
    "        data = readsheet.cell(i, 0).value\n",
    "        data = data.replace(\"'\", \"''\")\n",
    "        predicted = reloaded(data).numpy().decode('utf-8')\n",
    "        print(\"English{}=>Manipuri{}\".format(data, predicted))\n",
    "        writesheet.write(i, 1, predicted)\n",
    "        bengali_text = translate_bengali_scripts(predicted)\n",
    "        writesheet.write(i, 2, bengali_text)\n",
    "    writeb.save(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83de1701-5f4c-4eeb-9f26-9fe8c311c091",
   "metadata": {},
   "outputs": [
    {
     "ename": "XLRDError",
     "evalue": "Can't load sheets after releasing resources.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXLRDError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [15], line 6\u001b[0m, in \u001b[0;36minference\u001b[0;34m(file_location)\u001b[0m\n\u001b[1;32m      4\u001b[0m writeb \u001b[38;5;241m=\u001b[39m readb\n\u001b[1;32m      5\u001b[0m readsheet \u001b[38;5;241m=\u001b[39m readb\u001b[38;5;241m.\u001b[39msheet_by_index(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m writesheet \u001b[38;5;241m=\u001b[39m \u001b[43mwriteb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m nrows \u001b[38;5;241m=\u001b[39m readsheet\u001b[38;5;241m.\u001b[39mnrows\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# skip the first rows as it is header\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xlrd/book.py:698\u001b[0m, in \u001b[0;36mBook.get_sheet\u001b[0;34m(self, sh_number, update_pos)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sheet\u001b[39m(\u001b[38;5;28mself\u001b[39m, sh_number, update_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resources_released:\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m XLRDError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load sheets after releasing resources.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_pos:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sh_abs_posn[sh_number]\n",
      "\u001b[0;31mXLRDError\u001b[0m: Can't load sheets after releasing resources."
     ]
    }
   ],
   "source": [
    "inference(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4983a0e-0ee0-4495-8ddc-997bdbd0ed87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
